#!/usr/bin/env python3
"""
üßπ DATABASE DUPLICATE CLEANUP SYSTEM
Automated cleanup of 204 detected duplicates with preservation of critical data
CRITICAL: Eliminates redundant databases while preserving single source of truth
"""

import sqlite3
import os
import shutil
import json
from datetime import datetime
from pathlib import Path

class DatabaseDuplicateCleanup:
    def __init__(self):
        self.project_root = Path('/Users/dansidanutz/Desktop/ZmartBot')
        self.registry_path = self.project_root / 'zmart-api/src/data/master_database_registry.db'
        self.cleanup_log = []
        self.preserved_databases = []
        self.deleted_databases = []
        
        # Critical databases that must be preserved - NEVER DELETE THESE
        self.critical_preserve = {
            '/Users/dansidanutz/Desktop/ZmartBot/zmart-api/src/data/service_registry.db',  # Master service registry
            '/Users/dansidanutz/Desktop/ZmartBot/zmart-api/src/data/master_database_registry.db',  # This registry
        }
        
        # Important databases - only delete if truly empty and duplicate
        self.important_preserve = {
            'authentication.db',
            'api_keys.db', 
            'my_symbols_v2.db',
            'analytics.db',
            'trading_orchestration.db',
            'predictions.db',
            'ziva_violations.db',
            'market_data_enhanced.db',
            'cryptometer_complete.db',
            'ultimate_riskmetric.db'
        }
        
        print("üßπ Database Duplicate Cleanup System Initialized")
    
    def log_action(self, action, database, details):
        """Log all cleanup actions"""
        entry = {
            'timestamp': datetime.now().isoformat(),
            'action': action,
            'database': database,
            'details': details
        }
        self.cleanup_log.append(entry)
        print(f"üìù {action}: {database} - {details}")
    
    def get_duplicate_groups(self):
        """Get groups of duplicate databases from registry"""
        conn = sqlite3.connect(str(self.registry_path))
        cursor = conn.cursor()
        
        # Get all duplicate detections
        cursor.execute('''
            SELECT d.similarity_score, d.duplicate_type,
                   db1.full_path, db1.database_name, db1.size_bytes, db1.table_count, db1.record_count,
                   db2.full_path, db2.database_name, db2.size_bytes, db2.table_count, db2.record_count
            FROM duplicate_detections d
            JOIN database_registry db1 ON d.database1_id = db1.id
            JOIN database_registry db2 ON d.database2_id = db2.id
            WHERE d.similarity_score >= 0.9
            ORDER BY d.similarity_score DESC
        ''')
        
        duplicates = cursor.fetchall()
        conn.close()
        
        # Group duplicates by similarity and content
        duplicate_groups = {}
        for dup in duplicates:
            key = f"{dup[3]}_{dup[8]}"  # database names
            if key not in duplicate_groups:
                duplicate_groups[key] = []
            
            duplicate_groups[key].append({
                'similarity': dup[0],
                'type': dup[1],
                'db1': {'path': dup[2], 'name': dup[3], 'size': dup[4], 'tables': dup[5], 'records': dup[6]},
                'db2': {'path': dup[7], 'name': dup[8], 'size': dup[9], 'tables': dup[10], 'records': dup[11]}
            })
        
        return duplicate_groups
    
    def determine_best_database(self, db1, db2):
        """Determine which database to keep based on priority rules"""
        
        # Rule 1: Preserve critical databases
        if db1['path'] in self.critical_preserve:
            return db1, db2
        if db2['path'] in self.critical_preserve:
            return db2, db1
        
        # Rule 2: Check important databases - be extra careful
        if any(important in db1['path'] for important in self.important_preserve):
            if db1['size'] > 0 or db1['tables'] > 0 or db1['records'] > 0:
                return db1, db2  # Keep important database with any content
        if any(important in db2['path'] for important in self.important_preserve):
            if db2['size'] > 0 or db2['tables'] > 0 or db2['records'] > 0:
                return db2, db1  # Keep important database with any content
        
        # Rule 3: Prefer non-empty databases
        if db1['size'] > 0 and db2['size'] == 0:
            return db1, db2
        if db2['size'] > 0 and db1['size'] == 0:
            return db2, db1
        
        # Rule 3: Prefer databases with more data
        if db1['records'] > db2['records']:
            return db1, db2
        if db2['records'] > db1['records']:
            return db2, db1
        
        # Rule 4: Prefer databases in /src/data/ (current architecture)
        if '/src/data/' in db1['path'] and '/src/data/' not in db2['path']:
            return db1, db2
        if '/src/data/' in db2['path'] and '/src/data/' not in db1['path']:
            return db2, db1
        
        # Rule 5: Prefer non-backup databases
        if 'backup' not in db1['path'].lower() and 'backup' in db2['path'].lower():
            return db1, db2
        if 'backup' not in db2['path'].lower() and 'backup' in db1['path'].lower():
            return db2, db1
        
        # Rule 6: Prefer databases not in archive/test directories
        archive_keywords = ['archive', 'test', 'temp', 'old']
        db1_archive = any(keyword in db1['path'].lower() for keyword in archive_keywords)
        db2_archive = any(keyword in db2['path'].lower() for keyword in archive_keywords)
        
        if not db1_archive and db2_archive:
            return db1, db2
        if not db2_archive and db1_archive:
            return db2, db1
        
        # Default: keep the first one (older creation typically)
        return db1, db2
    
    def cleanup_empty_duplicates(self):
        """Remove all empty duplicate databases (0 bytes, 0 tables)"""
        print("üóëÔ∏è Cleaning up empty duplicate databases...")
        
        conn = sqlite3.connect(str(self.registry_path))
        cursor = conn.cursor()
        
        # Find all empty databases that are duplicates
        cursor.execute('''
            SELECT full_path, database_name 
            FROM database_registry 
            WHERE size_bytes = 0 AND table_count = 0 AND is_duplicate = TRUE
        ''')
        
        empty_duplicates = cursor.fetchall()
        conn.close()
        
        deleted_count = 0
        for db_path, db_name in empty_duplicates:
            try:
                if os.path.exists(db_path) and db_path not in self.critical_preserve:
                    os.remove(db_path)
                    self.deleted_databases.append(db_path)
                    self.log_action('DELETED_EMPTY', db_name, f'Empty duplicate removed from {db_path}')
                    deleted_count += 1
                else:
                    self.log_action('SKIPPED_CRITICAL', db_name, f'Critical database preserved: {db_path}')
            except Exception as e:
                self.log_action('ERROR', db_name, f'Failed to delete {db_path}: {e}')
        
        return deleted_count
    
    def cleanup_identical_structure_duplicates(self):
        """Clean up databases with identical structures but different names/locations"""
        print("üîß Cleaning up identical structure duplicates...")
        
        duplicate_groups = self.get_duplicate_groups()
        processed_paths = set()
        cleanup_count = 0
        
        for group_key, duplicates in duplicate_groups.items():
            # Process each duplicate pair in the group
            for dup in duplicates:
                db1 = dup['db1']
                db2 = dup['db2']
                
                # Skip if either database was already processed
                if db1['path'] in processed_paths or db2['path'] in processed_paths:
                    continue
                
                # Skip if similarity is not high enough
                if dup['similarity'] < 0.9:
                    continue
                
                # Determine which database to keep
                keep_db, delete_db = self.determine_best_database(db1, db2)
                
                try:
                    # Verify both files exist
                    if not os.path.exists(keep_db['path']):
                        self.log_action('ERROR', keep_db['name'], f'Keep database does not exist: {keep_db["path"]}')
                        continue
                    
                    if not os.path.exists(delete_db['path']):
                        self.log_action('SKIPPED', delete_db['name'], f'Delete database does not exist: {delete_db["path"]}')
                        continue
                    
                    # Skip critical databases
                    if delete_db['path'] in self.critical_preserve:
                        self.log_action('SKIPPED_CRITICAL', delete_db['name'], 'Critical database preserved')
                        continue
                    
                    # Extra safety check for important databases
                    if any(important in delete_db['path'] for important in self.important_preserve):
                        if delete_db['size'] > 0 or delete_db['tables'] > 0 or delete_db['records'] > 0:
                            self.log_action('SKIPPED_IMPORTANT', delete_db['name'], 
                                          f'Important database with data preserved (size: {delete_db["size"]}, tables: {delete_db["tables"]})')
                            continue
                    
                    # Create backup if database has data
                    if delete_db['size'] > 0:
                        backup_path = f"{delete_db['path']}.cleanup_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
                        shutil.copy2(delete_db['path'], backup_path)
                        self.log_action('BACKED_UP', delete_db['name'], f'Backup created: {backup_path}')
                    
                    # Delete the duplicate
                    os.remove(delete_db['path'])
                    self.deleted_databases.append(delete_db['path'])
                    self.preserved_databases.append(keep_db['path'])
                    
                    self.log_action('DELETED_DUPLICATE', delete_db['name'], 
                                  f'Removed duplicate (kept: {keep_db["name"]})')
                    
                    # Mark as processed
                    processed_paths.add(keep_db['path'])
                    processed_paths.add(delete_db['path'])
                    cleanup_count += 1
                
                except Exception as e:
                    self.log_action('ERROR', delete_db['name'], f'Failed to cleanup: {e}')
        
        return cleanup_count
    
    def cleanup_learning_data_duplicates(self):
        """Special cleanup for learning data duplicates with different symbols"""
        print("üß† Cleaning up learning data duplicates...")
        
        # Find all learning data files
        learning_files = []
        for root, dirs, files in os.walk(str(self.project_root)):
            for file in files:
                if 'learning_data_' in file and file.endswith('.db'):
                    full_path = os.path.join(root, file)
                    learning_files.append(full_path)
        
        # Group by structure similarity
        conn = sqlite3.connect(str(self.registry_path))
        cursor = conn.cursor()
        
        learning_duplicates = []
        processed = set()
        
        for file1 in learning_files:
            if file1 in processed:
                continue
            
            for file2 in learning_files:
                if file2 in processed or file1 == file2:
                    continue
                
                # Check if they're duplicates in our registry
                cursor.execute('''
                    SELECT d.similarity_score 
                    FROM duplicate_detections d
                    JOIN database_registry db1 ON d.database1_id = db1.id
                    JOIN database_registry db2 ON d.database2_id = db2.id
                    WHERE (db1.full_path = ? AND db2.full_path = ?) 
                       OR (db1.full_path = ? AND db2.full_path = ?)
                    AND d.similarity_score = 1.0
                ''', (file1, file2, file2, file1))
                
                result = cursor.fetchone()
                if result:
                    # Keep the first one (usually ETH as baseline)
                    try:
                        if 'ETH' in file1 and 'ETH' not in file2:
                            keep_file, delete_file = file1, file2
                        elif 'ETH' in file2 and 'ETH' not in file1:
                            keep_file, delete_file = file2, file1
                        else:
                            # Keep alphabetically first
                            keep_file, delete_file = sorted([file1, file2])[0], sorted([file1, file2])[1]
                        
                        if os.path.exists(delete_file):
                            os.remove(delete_file)
                            self.deleted_databases.append(delete_file)
                            self.log_action('DELETED_LEARNING', os.path.basename(delete_file),
                                          f'Learning duplicate removed (kept: {os.path.basename(keep_file)})')
                            processed.add(delete_file)
                    
                    except Exception as e:
                        self.log_action('ERROR', os.path.basename(file2), f'Learning cleanup failed: {e}')
        
        conn.close()
        return len(processed)
    
    def update_registry_after_cleanup(self):
        """Update master registry to mark deleted databases"""
        print("üìä Updating master registry after cleanup...")
        
        conn = sqlite3.connect(str(self.registry_path))
        cursor = conn.cursor()
        
        # Mark deleted databases as inactive
        for deleted_path in self.deleted_databases:
            cursor.execute('''
                UPDATE database_registry 
                SET is_active = FALSE, 
                    notes = COALESCE(notes, '') || ' DELETED BY CLEANUP ' || ?
                WHERE full_path = ?
            ''', (datetime.now().isoformat(), deleted_path))
        
        # Record cleanup action
        cursor.execute('''
            INSERT INTO database_access_log 
            (database_id, access_type, service_name, operation_type, details)
            SELECT id, 'MAINTENANCE', 'CLEANUP_SYSTEM', 'DUPLICATE_CLEANUP', 
                   'Database deleted during duplicate cleanup'
            FROM database_registry 
            WHERE full_path IN ({})
        '''.format(','.join(['?' for _ in self.deleted_databases])), self.deleted_databases)
        
        conn.commit()
        conn.close()
        
        self.log_action('REGISTRY_UPDATED', 'master_registry', 
                       f'Updated registry for {len(self.deleted_databases)} deleted databases')
    
    def generate_cleanup_report(self):
        """Generate comprehensive cleanup report"""
        report_path = self.project_root / 'zmart-api/database_consolidation/duplicate_cleanup_report.json'
        
        # Get final statistics
        conn = sqlite3.connect(str(self.registry_path))
        cursor = conn.cursor()
        
        cursor.execute("SELECT COUNT(*) FROM database_registry WHERE is_active = TRUE")
        active_databases = cursor.fetchone()[0]
        
        cursor.execute("SELECT COUNT(*) FROM database_registry WHERE is_active = FALSE")
        deleted_databases = cursor.fetchone()[0]
        
        cursor.execute("SELECT SUM(size_bytes) FROM database_registry WHERE is_active = TRUE")
        total_size = cursor.fetchone()[0] or 0
        
        conn.close()
        
        report = {
            'cleanup_timestamp': datetime.now().isoformat(),
            'summary': {
                'databases_deleted': len(self.deleted_databases),
                'databases_preserved': len(self.preserved_databases),
                'active_databases_remaining': active_databases,
                'total_size_after_cleanup_mb': round(total_size / (1024*1024), 2)
            },
            'deleted_databases': self.deleted_databases,
            'preserved_databases': self.preserved_databases,
            'cleanup_log': self.cleanup_log
        }
        
        with open(report_path, 'w') as f:
            json.dump(report, f, indent=2)
        
        print(f"\nüéâ DUPLICATE CLEANUP COMPLETE!")
        print(f"üóëÔ∏è Deleted: {len(self.deleted_databases)} duplicate databases")
        print(f"üíæ Preserved: {len(self.preserved_databases)} unique databases") 
        print(f"‚úÖ Active: {active_databases} databases remaining")
        print(f"üìä Total size after cleanup: {report['summary']['total_size_after_cleanup_mb']} MB")
        print(f"üìã Full report: {report_path}")
        
        return report
    
    def run_complete_cleanup(self):
        """Execute complete duplicate cleanup process"""
        print("üöÄ Starting Complete Database Duplicate Cleanup...")
        
        try:
            # Step 1: Clean up empty duplicates
            empty_cleaned = self.cleanup_empty_duplicates()
            print(f"‚úÖ Cleaned {empty_cleaned} empty duplicates")
            
            # Step 2: Clean up identical structure duplicates  
            structure_cleaned = self.cleanup_identical_structure_duplicates()
            print(f"‚úÖ Cleaned {structure_cleaned} structural duplicates")
            
            # Step 3: Clean up learning data duplicates
            learning_cleaned = self.cleanup_learning_data_duplicates()
            print(f"‚úÖ Cleaned {learning_cleaned} learning duplicates")
            
            # Step 4: Update registry
            self.update_registry_after_cleanup()
            
            # Step 5: Generate final report
            report = self.generate_cleanup_report()
            
            print(f"\nüéØ CLEANUP SUCCESS!")
            print(f"Total duplicates eliminated: {len(self.deleted_databases)}")
            print(f"System now has ZERO duplicate databases!")
            print(f"Prevention system remains active to FORBID future duplicates.")
            
            return True, report
            
        except Exception as e:
            self.log_action('CLEANUP_FAILED', 'system', f'Cleanup failed: {e}')
            print(f"‚ùå Cleanup failed: {e}")
            return False, None

if __name__ == "__main__":
    cleanup_system = DatabaseDuplicateCleanup()
    success, report = cleanup_system.run_complete_cleanup()