{
  "mdc_content": "# zmart-backtesting.mdc\n> Purpose: Comprehensive backtesting system for ZmartBot trading strategies with historical data analysis, performance metrics, and strategy optimization.\n\n## Service Overview\n**zmart_backtesting** is a sophisticated backend service that provides comprehensive backtesting capabilities for the ZmartBot trading system. It manages historical data analysis, strategy performance evaluation, risk metrics calculation, and strategy optimization to ensure trading strategies are thoroughly tested and optimized before live deployment.\n\n## Architecture\n**Type**: Backend Service (FastAPI-based)\n**Port**: 8013\n**Language**: Python 3.11+\n**Framework**: FastAPI, Pandas, NumPy, SciPy, SQLAlchemy\n**Database**: PostgreSQL (backtest results), Redis (caching)\n**Compute**: High-performance computing for large-scale backtesting\n\n### Core Components\n- **Backtesting Engine**: Core backtesting execution engine\n- **Data Manager**: Historical data loading and preprocessing\n- **Strategy Evaluator**: Strategy performance analysis and evaluation\n- **Risk Calculator**: Risk metrics calculation and analysis\n- **Optimization Engine**: Strategy parameter optimization\n- **Results Analyzer**: Performance results analysis and reporting\n- **Backtest API**: RESTful API for backtesting operations\n\n## API Endpoints\n\n### Backtesting Endpoints\n- `POST /api/v1/backtest/create` - Create new backtest\n- `GET /api/v1/backtest/{backtest_id}` - Get backtest details\n- `POST /api/v1/backtest/{backtest_id}/run` - Run backtest\n- `GET /api/v1/backtest/{backtest_id}/status` - Get backtest status\n- `GET /api/v1/backtest/{backtest_id}/results` - Get backtest results\n- `DELETE /api/v1/backtest/{backtest_id}` - Delete backtest\n- `GET /api/v1/backtest/list` - List all backtests\n\n### Strategy Management Endpoints\n- `POST /api/v1/strategies/create` - Create new strategy\n- `GET /api/v1/strategies/list` - List all strategies\n- `PUT /api/v1/strategies/{strategy_id}` - Update strategy\n- `DELETE /api/v1/strategies/{strategy_id}` - Delete strategy\n- `POST /api/v1/strategies/{strategy_id}/validate` - Validate strategy\n\n### Performance Analysis Endpoints\n- `GET /api/v1/performance/metrics` - Get performance metrics\n- `POST /api/v1/performance/compare` - Compare strategy performance\n- `GET /api/v1/performance/benchmark` - Get benchmark performance\n- `POST /api/v1/performance/analysis` - Run performance analysis\n\n### Risk Analysis Endpoints\n- `GET /api/v1/risk/metrics` - Get risk metrics\n- `POST /api/v1/risk/analysis` - Run risk analysis\n- `GET /api/v1/risk/stress-test` - Run stress tests\n- `POST /api/v1/risk/monte-carlo` - Run Monte Carlo simulation\n\n### Optimization Endpoints\n- `POST /api/v1/optimization/run` - Run strategy optimization\n- `GET /api/v1/optimization/{optimization_id}/status` - Get optimization status\n- `GET /api/v1/optimization/{optimization_id}/results` - Get optimization results\n- `POST /api/v1/optimization/parameter-scan` - Parameter scanning\n\n### Data Management Endpoints\n- `POST /api/v1/data/load` - Load historical data\n- `GET /api/v1/data/available` - Get available data sources\n- `POST /api/v1/data/validate` - Validate data quality\n- `GET /api/v1/data/statistics` - Get data statistics\n\n### Reporting Endpoints\n- `GET /api/v1/reports/performance` - Generate performance report\n- `GET /api/v1/reports/risk` - Generate risk report\n- `GET /api/v1/reports/comparison` - Generate comparison report\n- `POST /api/v1/reports/custom` - Generate custom report\n\n### Health & Status\n- `GET /health` - Service health check\n- `GET /ready` - Service readiness check\n- `GET /metrics` - Prometheus metrics\n- `GET /startup` - Startup probe endpoint\n\n## Backtesting Methodologies\n\n### Walk-Forward Analysis\n- **Purpose**: Out-of-sample testing with rolling windows\n- **Method**: Split data into training and testing periods\n- **Benefits**: Reduces overfitting, validates strategy robustness\n- **Parameters**: Window size, step size, minimum data requirements\n- **Output**: Performance metrics for each walk-forward period\n\n### Monte Carlo Simulation\n- **Purpose**: Random sampling for robustness testing\n- **Method**: Generate multiple random scenarios from historical data\n- **Benefits**: Tests strategy under various market conditions\n- **Parameters**: Number of simulations, confidence intervals\n- **Output**: Distribution of performance metrics\n\n### Cross-Validation\n- **Purpose**: K-fold validation for strategy reliability\n- **Method**: Split data into K folds, train on K-1, test on 1\n- **Benefits**: Comprehensive validation across different time periods\n- **Parameters**: Number of folds, validation metrics\n- **Output**: Cross-validation performance scores\n\n### Bootstrap Analysis\n- **Purpose**: Resampling for statistical significance\n- **Method**: Random sampling with replacement from historical data\n- **Benefits**: Estimates statistical significance of results\n- **Parameters**: Number of bootstrap samples, confidence levels\n- **Output**: Confidence intervals for performance metrics\n\n### Stress Testing\n- **Purpose**: Extreme market condition simulation\n- **Method**: Apply extreme market scenarios to strategy\n- **Benefits**: Tests strategy resilience under stress\n- **Parameters**: Stress scenarios, market shocks\n- **Output**: Performance under stress conditions\n\n### Scenario Analysis\n- **Purpose**: Different market scenario testing\n- **Method**: Test strategy under various market scenarios\n- **Benefits**: Understand strategy behavior in different conditions\n- **Parameters**: Market scenarios, economic conditions\n- **Output**: Scenario-specific performance metrics\n\n## Performance Metrics\n\n### Return Metrics\n- **Total Return**: Overall strategy performance over time period\n- **Annualized Return**: Annualized rate of return\n- **Compound Annual Growth Rate (CAGR)**: Geometric mean return\n- **Excess Return**: Return above benchmark or risk-free rate\n- **Risk-Adjusted Return**: Return adjusted for risk taken\n\n### Risk-Adjusted Metrics\n- **Sharpe Ratio**: Return per unit of risk (volatility)\n- **Sortino Ratio**: Return per unit of downside risk\n- **Calmar Ratio**: Annual return to maximum drawdown ratio\n- **Information Ratio**: Excess return to tracking error ratio\n- **Treynor Ratio**: Excess return to beta ratio\n\n### Risk Metrics\n- **Maximum Drawdown**: Largest peak-to-trough decline\n- **Value at Risk (VaR)**: Potential loss at confidence level\n- **Expected Shortfall**: Average loss beyond VaR\n- **Volatility**: Standard deviation of returns\n- **Downside Deviation**: Negative return volatility\n\n### Trading Metrics\n- **Win Rate**: Percentage of profitable trades\n- **Profit Factor**: Gross profit to gross loss ratio\n- **Average Trade**: Mean profit/loss per trade\n- **Average Win**: Average profit on winning trades\n- **Average Loss**: Average loss on losing trades\n- **Largest Win**: Largest single winning trade\n- **Largest Loss**: Largest single losing trade\n\n### Advanced Metrics\n- **Risk of Ruin**: Probability of account depletion\n- **Kelly Criterion**: Optimal position sizing\n- **Ulcer Index**: Measure of downside risk\n- **Gain to Pain Ratio**: Total gain to total pain ratio\n- **Recovery Factor**: Total return to maximum drawdown ratio\n\n## Risk Metrics\n\n### Value at Risk (VaR)\n- **Definition**: Maximum expected loss at confidence level\n- **Calculation**: Statistical method based on return distribution\n- **Confidence Levels**: 95%, 99%, 99.9%\n- **Time Horizons**: 1 day, 1 week, 1 month, 1 year\n- **Methods**: Historical, Parametric, Monte Carlo\n\n### Expected Shortfall (ES)\n- **Definition**: Average loss beyond VaR threshold\n- **Calculation**: Conditional expectation of losses\n- **Advantages**: More conservative than VaR\n- **Applications**: Risk management, capital allocation\n- **Regulatory**: Basel III compliant risk measure\n\n### Volatility Analysis\n- **Historical Volatility**: Standard deviation of returns\n- **Implied Volatility**: Market expectation of future volatility\n- **Realized Volatility**: Actual volatility over time period\n- **Volatility Clustering**: Persistence of volatility\n- **Volatility Regimes**: Different volatility states\n\n### Correlation Analysis\n- **Asset Correlation**: Correlation between assets\n- **Strategy Correlation**: Correlation with benchmarks\n- **Rolling Correlation**: Time-varying correlation\n- **Correlation Breakdown**: Correlation during stress\n- **Diversification Benefits**: Portfolio diversification effects\n\n### Distribution Analysis\n- **Skewness**: Return distribution asymmetry\n- **Kurtosis**: Return distribution tail behavior\n- **Normality Tests**: Test for normal distribution\n- **Fat Tails**: Extreme event probability\n- **Tail Dependence**: Dependence in extreme events\n\n## Optimization Algorithms\n\n### Genetic Algorithm\n- **Purpose**: Evolutionary strategy optimization\n- **Method**: Natural selection and genetic operations\n- **Parameters**: Population size, mutation rate, crossover rate\n- **Advantages**: Global optimization, handles non-linear problems\n- **Applications**: Strategy parameter tuning, portfolio optimization\n\n### Particle Swarm Optimization\n- **Purpose**: Swarm intelligence optimization\n- **Method**: Particle movement in search space\n- **Parameters**: Swarm size, velocity, inertia weight\n- **Advantages**: Fast convergence, simple implementation\n- **Applications**: Technical indicator optimization\n\n### Bayesian Optimization\n- **Purpose**: Probabilistic parameter tuning\n- **Method**: Gaussian process regression and acquisition functions\n- **Parameters**: Prior distributions, acquisition function\n- **Advantages**: Sample efficient, handles noise\n- **Applications**: Expensive function optimization\n\n### Grid Search\n- **Purpose**: Exhaustive parameter search\n- **Method**: Systematic parameter space exploration\n- **Parameters**: Parameter ranges, step sizes\n- **Advantages**: Guaranteed to find global optimum\n- **Disadvantages**: Computationally expensive\n\n### Random Search\n- **Purpose**: Stochastic parameter exploration\n- **Method**: Random sampling from parameter space\n- **Parameters**: Number of trials, parameter distributions\n- **Advantages**: Simple, parallelizable\n- **Applications**: Initial parameter exploration\n\n### Simulated Annealing\n- **Purpose**: Global optimization with local search\n- **Method**: Temperature-based acceptance probability\n- **Parameters**: Initial temperature, cooling rate\n- **Advantages**: Escapes local optima\n- **Applications**: Complex optimization problems\n\n## Data Management\n\n### Historical Data Requirements\n- **Price Data**: OHLCV (Open, High, Low, Close, Volume)\n- **Time Resolution**: 1 minute, 5 minutes, 1 hour, 1 day\n- **Data Quality**: Clean, adjusted, split-adjusted data\n- **Coverage**: Sufficient historical depth for backtesting\n- **Completeness**: No missing data points\n\n### Market Data\n- **Volume Data**: Trading volume and turnover\n- **Bid/Ask Spreads**: Spread information for slippage\n- **Market Depth**: Order book data for impact analysis\n- **Tick Data**: High-frequency price movements\n- **Corporate Actions**: Dividends, splits, mergers\n\n### Economic Data\n- **Interest Rates**: Risk-free rates, yield curves\n- **Inflation Data**: CPI, PPI, inflation expectations\n- **GDP Data**: Economic growth indicators\n- **Employment Data**: Unemployment rates, job reports\n- **Central Bank Data**: Monetary policy indicators\n\n### Alternative Data\n- **News Data**: Market sentiment and news events\n- **Social Media**: Social sentiment indicators\n- **Satellite Data**: Economic activity indicators\n- **Weather Data**: Weather impact on markets\n- **ESG Data**: Environmental, social, governance factors\n\n### Data Processing\n- **Data Cleaning**: Remove outliers, fill missing values\n- **Data Validation**: Check for data quality issues\n- **Data Alignment**: Align different data sources\n- **Data Normalization**: Standardize data formats\n- **Data Storage**: Efficient storage and retrieval\n\n## Integration\n\n### Dependencies\n- **zmart-api**: Trading data and strategy execution\n- **zmart_technical_analysis**: Technical indicator calculations\n- **zmart_risk_management**: Risk metrics and limits\n- **zmart_data_warehouse**: Historical data storage and retrieval\n\n### External Integrations\n- **Data Providers**: Market data vendors and APIs\n- **Strategy Frameworks**: Backtrader, Zipline, Quantopian\n- **Analytics Libraries**: Pandas, NumPy, SciPy, Scikit-learn\n- **Visualization Tools**: Matplotlib, Plotly, Dash\n- **Reporting Tools**: Jupyter notebooks, automated reports\n\n### Data Flow\n1. **Data Ingestion**: Load historical data from sources\n2. **Data Processing**: Clean, validate, and prepare data\n3. **Strategy Definition**: Define trading rules and parameters\n4. **Backtest Execution**: Run strategy on historical data\n5. **Performance Calculation**: Calculate metrics and statistics\n6. **Risk Analysis**: Analyze risk metrics and drawdowns\n7. **Optimization**: Optimize strategy parameters\n8. **Validation**: Out-of-sample testing and validation\n9. **Reporting**: Generate comprehensive reports\n\n## Configuration\n\n### Environment Variables\n```bash\nBACKTEST_DB_HOST=localhost\nBACKTEST_DB_PORT=5432\nBACKTEST_DB_NAME=zmart_backtesting\nBACKTEST_DB_USER=backtest_user\nBACKTEST_DB_PASSWORD=secure_password\nREDIS_HOST=localhost\nREDIS_PORT=6379\nDATA_WAREHOUSE_URL=http://localhost:8015\nTECHNICAL_ANALYSIS_URL=http://localhost:8011\nRISK_MANAGEMENT_URL=http://localhost:8010\nAPI_RATE_LIMIT=500\nBACKTEST_TIMEOUT=300\nOPTIMIZATION_TIMEOUT=600\nMAX_CONCURRENT_BACKTESTS=10\nPARALLEL_PROCESSING=true\nCACHE_ENABLED=true\n```\n\n### Backtesting Configuration\n```yaml\nbacktesting_settings:\n  default_commission: 0.001  # 0.1% commission\n  default_slippage: 0.0005   # 0.05% slippage\n  initial_capital: 100000     # $100,000 initial capital\n  position_sizing: 0.02       # 2% position size\n  max_positions: 10           # Maximum 10 concurrent positions\n  \n  data_settings:\n    default_timeframe: 1h     # 1-hour timeframe\n    min_data_points: 1000     # Minimum data points required\n    data_quality_threshold: 0.95  # 95% data quality required\n    \n  execution_settings:\n    order_type: market        # Market order execution\n    fill_strategy: next_bar   # Fill on next bar\n    partial_fills: true       # Allow partial fills\n    \n  risk_settings:\n    max_drawdown: 0.20        # 20% maximum drawdown\n    stop_loss: 0.05           # 5% stop loss\n    take_profit: 0.10         # 10% take profit\n    \noptimization_settings:\n  algorithm: genetic          # Genetic algorithm\n  population_size: 50         # Population size\n  generations: 100            # Number of generations\n  mutation_rate: 0.1          # 10% mutation rate\n  crossover_rate: 0.8         # 80% crossover rate\n  \n  parameter_ranges:\n    rsi_period: [10, 30]      # RSI period range\n    macd_fast: [8, 16]        # MACD fast period\n    macd_slow: [20, 40]       # MACD slow period\n    bollinger_period: [15, 25] # Bollinger Bands period\n    \nreporting_settings:\n  generate_charts: true       # Generate performance charts\n  export_results: true        # Export results to files\n  email_reports: false        # Email reports\n  dashboard_integration: true # Dashboard integration\n```\n\n## Monitoring\n\n### Health Checks\n- **Liveness**: Service is running and responsive\n- **Readiness**: Service is ready to handle backtesting requests\n- **Startup**: Service has completed initialization\n\n### Metrics\n- **Backtest Execution Rate**: Backtests completed per hour\n- **Execution Time**: Average backtest execution time\n- **Success Rate**: Percentage of successful backtests\n- **Resource Usage**: CPU, memory, and storage usage\n- **Queue Depth**: Number of pending backtests\n- **Error Rate**: Backtest execution errors\n- **Optimization Progress**: Optimization algorithm progress\n\n### Logging\n- **Structured Logs**: JSON format for easy parsing\n- **Log Levels**: DEBUG, INFO, WARNING, ERROR, CRITICAL\n- **Backtest Events**: All backtest creation, execution, and completion events\n- **Performance Events**: Performance calculation and analysis events\n- **Optimization Events**: Optimization algorithm events and progress\n- **Data Events**: Data loading, validation, and processing events\n\n## Security\n\n### Authentication\n- **Service Token**: Required for all API access\n- **API Key Validation**: Secure API key management\n- **Rate Limiting**: 500 requests per minute\n- **Burst Protection**: 50 requests burst limit\n\n### Data Protection\n- **Encryption**: All backtest data encrypted in transit and at rest\n- **Access Control**: Role-based access to backtest results\n- **Audit Logging**: All backtest operations logged\n- **Data Retention**: Configurable data retention policies\n\n### API Security\n- **HTTPS Only**: All API communication over HTTPS\n- **Input Validation**: Strict input validation and sanitization\n- **SQL Injection Protection**: Parameterized queries\n- **XSS Protection**: Output encoding and validation\n\n## Performance\n\n### SLOs (Service Level Objectives)\n- **Availability**: 99.5% uptime\n- **Response Time**: < 5 seconds for backtest creation\n- **Execution Time**: < 5 minutes for standard backtests\n- **Throughput**: 100 backtests per hour\n- **Optimization Time**: < 30 minutes for standard optimizations\n\n### Resource Limits\n- **Memory**: 2GB maximum usage\n- **CPU**: 1.0 CPU cores\n- **Connections**: 100 concurrent database connections\n- **Request Timeout**: 300 seconds maximum\n- **Storage**: 10GB for backtest results\n\n### Optimization\n- **Parallel Processing**: Multi-threaded backtest execution\n- **Caching**: Redis-based caching for frequently accessed data\n- **Data Compression**: Compressed storage for historical data\n- **Database Optimization**: Indexed queries and connection pooling\n- **Load Balancing**: Distributed processing for large backtests\n\n## Failure Modes\n\n### Historical Data Unavailable\n**Symptoms**: Cannot access historical data for backtesting\n**Impact**: Backtests cannot be executed\n**Remediation**: Check data warehouse connectivity, verify data availability\n**Prevention**: Data availability monitoring, backup data sources\n\n### Backtest Execution Failed\n**Symptoms**: Backtest execution errors or timeouts\n**Impact**: Strategy validation delayed\n**Remediation**: Check strategy configuration, restart backtesting engine\n**Prevention**: Strategy validation, timeout management\n\n### Performance Calculation Failed\n**Symptoms**: Performance metrics calculation errors\n**Impact**: Incomplete backtest results\n**Remediation**: Check calculation algorithms, restart metrics engine\n**Prevention**: Algorithm validation, error handling\n\n### Optimization Timeout\n**Symptoms**: Optimization algorithms timeout\n**Impact**: Strategy optimization incomplete\n**Remediation**: Increase timeout limits, simplify optimization\n**Prevention**: Algorithm tuning, resource allocation\n\n### Memory Exhaustion\n**Symptoms**: High memory usage during large backtests\n**Impact**: Service crashes or slow performance\n**Remediation**: Increase memory limits, optimize data processing\n**Prevention**: Memory monitoring, data chunking\n\n## Dependencies\n\n### Required Services\n- **zmart-api**: Trading data and strategy execution\n- **zmart_technical_analysis**: Technical indicator calculations\n- **zmart_risk_management**: Risk metrics and limits\n- **zmart_data_warehouse**: Historical data storage and retrieval\n\n### External Dependencies\n- **PostgreSQL**: Backtest results and configuration storage\n- **Redis**: Caching and queue management\n- **Data Providers**: Market data vendors and APIs\n- **Analytics Libraries**: Pandas, NumPy, SciPy, Scikit-learn\n- **Optimization Libraries**: SciPy, DEAP, Optuna\n\n### System Requirements\n- **Python 3.11+**: Runtime environment\n- **FastAPI**: Web framework\n- **Pandas**: Data manipulation and analysis\n- **NumPy**: Numerical computing\n- **SciPy**: Scientific computing and optimization\n- **SQLAlchemy**: Database ORM\n\n## Orchestration\n\n### Auto Start\n- **Enabled**: Yes\n- **Dependencies**: zmart-api, zmart_technical_analysis, zmart_risk_management, zmart_data_warehouse\n- **Startup Order**: After dependencies are ready\n\n### Restart Policy\n- **Policy**: Always restart on failure\n- **Max Restarts**: 3 attempts\n- **Restart Delay**: 15 seconds between attempts\n- **Backoff Strategy**: Exponential backoff\n\n### Health Monitoring\n- **Check Interval**: 15 seconds\n- **Timeout**: 10 seconds\n- **Failure Threshold**: 3 consecutive failures\n- **Recovery Threshold**: 1 successful check\n\n## Rollback\n\n### Snapshot Requirements\n- **Required**: Yes\n- **Frequency**: Before major strategy updates\n- **Data Included**: Backtest results, strategy configurations, optimization results\n- **Retention**: 90 days of snapshots\n\n### Rollback Strategy\n- **Strategy**: Immediate rollback on critical failures\n- **Data Consistency**: Full data consistency check\n- **Validation**: Backtest system validation after rollback\n- **Notification**: Alert stakeholders of rollback\n\n### Data Protection\n- **Backup Strategy**: Daily automated backups\n- **Recovery Time**: < 10 minutes\n- **Data Integrity**: Checksums and validation\n- **Point-in-Time Recovery**: Support for specific timestamps\n\n## Load Balancing\n\n### Configuration\n- **Enabled**: No (single instance)\n- **Pool**: null\n- **Reason**: Backtesting requires state consistency and large memory usage\n\n### Scaling Considerations\n- **Horizontal Scaling**: Not supported (stateful service)\n- **Vertical Scaling**: Memory and CPU upgrades\n- **Read Replicas**: Database read replicas for queries\n- **Caching**: Redis-based caching for performance\n\n## Known Issues\n\n### Long Backtest Execution Times\n**Issue**: Long backtest execution times for large datasets\n**Status**: Monitoring\n**Impact**: Delayed strategy validation\n**Workaround**: Implement parallel processing and data chunking\n**Long-term Fix**: Distributed computing and optimization\n\n### Memory Usage\n**Issue**: High memory usage during large backtests\n**Status**: Investigating\n**Impact**: Service performance degradation\n**Workaround**: Increase memory limits, optimize data structures\n**Long-term Fix**: Memory-efficient algorithms and data processing\n\n### Optimization Convergence\n**Issue**: Optimization algorithms may not converge to optimal solution\n**Status**: Monitoring\n**Impact**: Suboptimal strategy parameters\n**Workaround**: Adjust optimization parameters, multiple runs\n**Long-term Fix**: Advanced optimization algorithms and parameter tuning\n\n## Status\n✅ **DISCOVERED** - Backtesting system with comprehensive strategy evaluation and optimization capabilities\n\n## Changelog\n- **1.0.0** (2025-08-25): Initial backtesting system implementation\n  - Historical data analysis and processing\n  - Strategy performance evaluation\n  - Risk metrics calculation\n  - Optimization algorithms\n  - Comprehensive reporting and analytics\n\n---\ndescription: \"Backtesting system for ZmartBot trading strategies\"\nglobs:\n  - \"**/*.py\"\n  - \"**/*.yaml\"\n  - \"**/*.yml\"\nalwaysApply: true\nseverity: \"info\"\ntags: [\"backtesting\", \"backend\", \"strategy-evaluation\", \"optimization\"]\nupdated: \"2025-08-25\"\n",
  "validation": {
    "schema_compliance": true,
    "content_completeness": true,
    "technical_accuracy": true,
    "format_correctness": true
  },
  "metadata": {
    "generated_at": "2025-08-25T01:40:00Z",
    "model": "gpt-5",
    "input_hash": "sha256-backtesting-service",
    "context_files": ["backtesting_context.json", "service.yaml"]
  }
}
