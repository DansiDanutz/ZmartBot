version: '3.8'

networks:
  diana-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16

volumes:
  postgres-data:
    driver: local
  redis-data:
    driver: local
  rabbitmq-data:
    driver: local
  prometheus-data:
    driver: local
  grafana-data:
    driver: local

services:
  # ===========================
  # CORE DATA LAYER
  # ===========================
  postgres:
    image: postgres:15-alpine
    container_name: diana-postgres
    restart: unless-stopped
    environment:
      POSTGRES_DB: zmart_core
      POSTGRES_USER: zmart
      POSTGRES_PASSWORD: zmart
      PGDATA: /var/lib/postgresql/data/pgdata
    volumes:
      - postgres-data:/var/lib/postgresql/data
      - ./postgres/init:/docker-entrypoint-initdb.d
    ports:
      - "5432:5432"
    networks:
      - diana-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U zmart -d zmart_core"]
      interval: 10s
      timeout: 5s
      retries: 5
    command: >
      postgres 
      -c max_connections=200
      -c shared_buffers=256MB
      -c effective_cache_size=1GB
      -c work_mem=4MB

  redis:
    image: redis:7-alpine
    container_name: diana-redis
    restart: unless-stopped
    volumes:
      - redis-data:/data
      - ./redis/redis.conf:/usr/local/etc/redis/redis.conf
    ports:
      - "6379:6379"
    networks:
      - diana-network
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    command: redis-server /usr/local/etc/redis/redis.conf

  # ===========================
  # EVENT-DRIVEN MESSAGING
  # ===========================
  rabbitmq:
    image: rabbitmq:3.12-management-alpine
    container_name: diana-rabbitmq
    restart: unless-stopped
    environment:
      RABBITMQ_DEFAULT_USER: zmart_user
      RABBITMQ_DEFAULT_PASS: zmart_rabbitmq_password
      RABBITMQ_DEFAULT_VHOST: zmart_vhost
      RABBITMQ_ERLANG_COOKIE: zmart_erlang_secret_cookie
    volumes:
      - rabbitmq-data:/var/lib/rabbitmq
      - ./rabbitmq/enabled_plugins:/etc/rabbitmq/enabled_plugins
      - ./rabbitmq/rabbitmq.conf:/etc/rabbitmq/rabbitmq.conf
    ports:
      - "5672:5672"   # AMQP
      - "15672:15672" # Management UI
    networks:
      - diana-network
    healthcheck:
      test: ["CMD", "rabbitmq-diagnostics", "check_port_connectivity"]
      interval: 30s
      timeout: 10s
      retries: 5

  # ===========================
  # OBSERVABILITY STACK
  # ===========================
  jaeger:
    image: jaegertracing/all-in-one:1.50
    container_name: diana-jaeger
    restart: unless-stopped
    environment:
      COLLECTOR_OTLP_ENABLED: true
      COLLECTOR_ZIPKIN_HOST_PORT: ":9411"
    ports:
      - "16686:16686" # Jaeger UI
      - "14268:14268" # HTTP collector
      - "14250:14250" # gRPC collector
      - "9411:9411"   # Zipkin collector
    networks:
      - diana-network
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:16686/"]
      interval: 30s
      timeout: 10s
      retries: 3

  otel-collector:
    image: otel/opentelemetry-collector-contrib:0.88.0
    container_name: diana-otel-collector
    restart: unless-stopped
    command: ["--config=/etc/otel-collector-config.yaml"]
    volumes:
      - ./otel/otel-collector-config.yaml:/etc/otel-collector-config.yaml
    ports:
      - "1888:1888"   # pprof extension
      - "8888:8888"   # Prometheus metrics
      - "8889:8889"   # Prometheus exporter
      - "13133:13133" # health_check extension
      - "4317:4317"   # OTLP gRPC receiver
      - "4318:4318"   # OTLP HTTP receiver
      - "55679:55679" # zpages extension
    networks:
      - diana-network
    depends_on:
      - jaeger
      - prometheus
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:13133/"]
      interval: 30s
      timeout: 10s
      retries: 3

  prometheus:
    image: prom/prometheus:v2.47.0
    container_name: diana-prometheus
    restart: unless-stopped
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=200h'
      - '--web.enable-lifecycle'
      - '--web.enable-admin-api'
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus-data:/prometheus
    ports:
      - "9090:9090"
    networks:
      - diana-network
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3

  grafana:
    image: grafana/grafana:10.1.0
    container_name: diana-grafana
    restart: unless-stopped
    environment:
      GF_SECURITY_ADMIN_USER: admin
      GF_SECURITY_ADMIN_PASSWORD: admin
      GF_USERS_ALLOW_SIGN_UP: false
      GF_INSTALL_PLUGINS: grafana-clock-panel,grafana-simple-json-datasource
    volumes:
      - grafana-data:/var/lib/grafana
      - ./grafana/provisioning:/etc/grafana/provisioning
      - ./grafana/dashboards:/var/lib/grafana/dashboards
    ports:
      - "3000:3000"
    networks:
      - diana-network
    depends_on:
      - prometheus
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:3000/api/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3

  # ===========================
  # SERVICE MESH & DISCOVERY
  # ===========================
  consul:
    image: consul:1.16.0
    container_name: diana-consul
    restart: unless-stopped
    command: agent -server -ui -node=server-1 -bootstrap-expect=1 -client=0.0.0.0
    environment:
      CONSUL_BIND_INTERFACE: eth0
    volumes:
      - ./consul/config:/consul/config
    ports:
      - "8500:8500" # HTTP API
      - "8600:8600" # DNS
    networks:
      - diana-network
    healthcheck:
      test: ["CMD", "consul", "members"]
      interval: 30s
      timeout: 10s
      retries: 3

  # ===========================
  # CONFIGURATION SERVER
  # ===========================
  config-server:
    image: alpine:3.18
    container_name: diana-config-server
    restart: unless-stopped
    command: |
      sh -c "
        apk add --no-cache python3 py3-pip py3-fastapi py3-uvicorn py3-yaml py3-consul &&
        python3 -c \"
        import uvicorn
        from fastapi import FastAPI, HTTPException
        from typing import Dict, Any
        import yaml
        import os
        import consul
        
        app = FastAPI(title='Diana Config Server', version='1.0.0')
        
        @app.get('/health')
        async def health():
            return {'status': 'healthy', 'service': 'config-server'}
        
        @app.get('/config/{service}')
        async def get_config(service: str):
            try:
                with open(f'/config/{service}.yml', 'r') as f:
                    config = yaml.safe_load(f)
                return {'success': True, 'config': config}
            except FileNotFoundError:
                raise HTTPException(404, f'Config for {service} not found')
        
        uvicorn.run(app, host='0.0.0.0', port=8080)
        \"
      "
    volumes:
      - ./config:/config
    ports:
      - "8080:8080"
    networks:
      - diana-network
    depends_on:
      - consul
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # ===========================
  # LOAD BALANCER & API GATEWAY
  # ===========================
  nginx:
    image: nginx:1.25-alpine
    container_name: diana-gateway
    restart: unless-stopped
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf
      - ./nginx/conf.d:/etc/nginx/conf.d
    ports:
      - "80:80"
      - "443:443"
    networks:
      - diana-network
    depends_on:
      - consul
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost/health"]
      interval: 30s
      timeout: 10s
      retries: 3